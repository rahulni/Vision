{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWh-8fCxQYqn"
   },
   "source": [
    "# **EXERCISE SOLUTION:**\n",
    "- Write the class for sigmoid activation function and define function for forward & backward propagation and retrain the model with sigmoid activation(replace softmax) in the output layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEgAy2096XSs"
   },
   "outputs": [],
   "source": [
    "#Importing required Modules\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItIAdJbV6i_o"
   },
   "outputs": [],
   "source": [
    "#Base layer class to specify the Layer properites for our Reshape and Convolution Layer to inherit\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        # TODO: return output\n",
    "        pass\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        # TODO: update parameters and return input gradient\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVOQ8Lpe6qIF"
   },
   "outputs": [],
   "source": [
    "class Convolutional(Layer):\n",
    "    def __init__(self, input_shape, kernel_size, depth):\n",
    "        #Input_shape is 3 dimensional (dxhxw), input depth representing no. of image input channels, input_height = image height and input_width = image width\n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        #Depth represents the number of kernels of our convolutional layer\n",
    "        self.depth = depth\n",
    "        self.input_shape = input_shape\n",
    "        #Number of channels in the image, i.e. 3 in a RGB image, 2 in a grayscale image\n",
    "        self.input_depth = input_depth\n",
    "        #Calculating our conv. layer output of 3 dimensions, first dim = number of filters/kernels, \n",
    "        #second dim = height of the output matrix after applying convolution i.e input image height - kernel size + 1 by rule\n",
    "        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
    "        #Kernels shape specifies the shape of the kernels produced, 4 dimensions depth = no. of kernels, input_depth = image channels, kernel_size = kernel dimension \n",
    "        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size)\n",
    "        #Initalizing the Kernels weights randomly\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)\n",
    "        #Initializing the biases randomly\n",
    "        self.biases = np.random.rand(*self.output_shape)\n",
    "\n",
    "    #Forward pass, takes input and computes the output by applying the above convolution \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        #Inititialize output matrix with output_shape \n",
    "        self.output = np.zeros(self.output_shape)\n",
    "\n",
    "        #Two nested for loops for first traversing all filters (depth), then all channels (input_depth) in every input image\n",
    "        for i in range(self.depth):\n",
    "            for j in range(self.input_depth):\n",
    "                #Output is calculated by adding the biases of the layer with the Cross Correlation between image and the kernel, valid stands for no padding in our correlation calculation inputs\n",
    "                self.output[i] = self.biases[i]+ signal.correlate2d(self.input[j], self.kernels[i, j], \"valid\")\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        #Intializing the gradient of the kernels as zeros\n",
    "        kernels_gradient = np.zeros(self.kernels_shape)\n",
    "        #Intializing the gradient of the input as zeros\n",
    "        input_gradient = np.zeros(self.input_shape)\n",
    "\n",
    "        #Nested for loop for updating the gradients first traversing all filters (depth), then all channels (input_depth) in every input image to update the gradients of kernels and inputs\n",
    "        for i in range(self.depth):\n",
    "            for j in range(self.input_depth):\n",
    "                #Calculate kernels gradient in every i and j index in the kernel, compute correlation between image and output graident\n",
    "                kernels_gradient[i, j] = signal.correlate2d(self.input[j], output_gradient[i], \"valid\")\n",
    "                #Calculate input gradient by sliding the kernel on the output gradient matrix\n",
    "                input_gradient[j] += signal.convolve2d(output_gradient[i], self.kernels[i, j], \"full\")\n",
    "\n",
    "        #Update the kernels and biases w.r.t. learned features (stored in gradients)\n",
    "        #Gradients are multiplied with the learning rate to update the kernels and biases  \n",
    "        self.kernels -= learning_rate * kernels_gradient\n",
    "        self.biases -= learning_rate * np.sum(output_gradient)\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukfb6Sf36yDx"
   },
   "outputs": [],
   "source": [
    "#Base Activation class to specify the default properties of the The Activation Layer from which we will derive our Activation functions ReLU, Softmax, TanH\n",
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        #Calculation for the activation function\n",
    "        self.activation = activation\n",
    "        #Calculation for derivative of activation function which will be handy while backpropagation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.input)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        #Implement the backward function for the activation, by multiplying the output gradient and the derivative of the loss\n",
    "        return np.multiply(output_gradient, self.activation_prime(self.input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2j7dWOUmhHtE"
   },
   "outputs": [],
   "source": [
    "class ReLU(Activation):\n",
    "  def __init__(self):\n",
    "    def relu(x):\n",
    "        return np.where(x > 0, x, 0)\n",
    "\n",
    "    def relu_prime(x):\n",
    "        return np.where(x <= 0, 0, 1)\n",
    "\n",
    "    super().__init__(relu, relu_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwYKpkIaiuXf"
   },
   "outputs": [],
   "source": [
    "class TanH(Activation):\n",
    "    def __init__(self):\n",
    "        def tanh(x):\n",
    "            return np.tanh(x)\n",
    "\n",
    "        def tanh_prime(x):\n",
    "            return 1 - np.tanh(x) ** 2\n",
    "\n",
    "        super().__init__(tanh, tanh_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCjli7pq6oZW"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "##**Sigmoid Activation**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msItpFfziuXf"
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "\n",
    "        def sigmoid_prime(x):\n",
    "            s = sigmoid(x)\n",
    "            return s * (1 - s)\n",
    "\n",
    "        super().__init__(sigmoid, sigmoid_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBGv59lm6sh7"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdJICNZP8rji"
   },
   "outputs": [],
   "source": [
    "class MaxPool(Layer):\n",
    "    def __init__(self, input_shape, kernel_size, depth, stride):\n",
    "        #Input_shape is 3 dimensional (dxhxw), input depth representing no. of image input channels, input_height = image height and input_width = image width\n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        #Specifying the shape of input\n",
    "        self.input_shape = input_shape\n",
    "        #Specfiying the kernel size of our MaxPool operation\n",
    "        self.kernel_size = kernel_size\n",
    "        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size)\n",
    "        #Specifying the depth/no. of filters\n",
    "        self.depth = depth\n",
    "        #Specifying the depth/channels of our input\n",
    "        self.input_depth = input_depth\n",
    "        #initializing the kernels with random values of shape (kernels_shape)\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)\n",
    "        self.stride = stride\n",
    "        self.input_height, self.input_width = input_height, input_width\n",
    "\n",
    "    #forward method to perform the MaxPool operation on the input \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        \n",
    "        KH = 1 + (self.input_height - self.kernel_size) // self.stride\n",
    "        KW = 1 + (self.input_width - self.kernel_size) // self.stride\n",
    "        self.output = np.zeros((self.input_depth, KH, KW))\n",
    "\n",
    "        for depth in range(self.input_depth):\n",
    "            for r in range(0, self.input_height-1, self.stride):\n",
    "                for c in range(0, self.input_width-1, self.stride):\n",
    "                    self.output[depth, r//self.stride, c//self.stride] = np.max(self.input[depth, r:r+self.kernel_size, c:c+self.kernel_size])\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate): \n",
    "        # Two nested for loops for first traversing all filters (depth), then all channels (input_depth) in every input image\n",
    "        self.output_gradient = np.zeros(self.input_shape)\n",
    "        dx = np.zeros(self.input_shape)\n",
    "        for depth in range(self.input_depth):\n",
    "            for r in range(0, self.input_height-1, self.stride):\n",
    "                for c in range(0, self.input_width-1, self.stride):\n",
    "                    grad_pool = self.output[depth, r*self.stride:r*self.stride+self.kernel_size, c*self.stride:c*self.stride+self.kernel_size]\n",
    "                    mask = (grad_pool == np.max(grad_pool))\n",
    "                    dx[depth, r*self.stride:r*self.stride+self.kernel_size, c*self.stride : c*self.stride+self.kernel_size] = mask*self.output_gradient[depth, r, c]\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6Jw_I0I6MfM"
   },
   "outputs": [],
   "source": [
    "class Reshape(Layer):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "    #Specify input shape and out shape in the constructor\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    #Forward reshapes the input to the output shape\n",
    "    def forward(self, input):\n",
    "        return np.reshape(input, self.output_shape)\n",
    "\n",
    "    #Backward reshapes the output to the input shape\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.reshape(output_gradient, self.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQWT8WsV7cu0"
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        #Defining our weights matrix shape\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        #Defining our bias matrix size\n",
    "        self.bias = np.random.randn(output_size, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        #Implementing the equation above by summing bias of the dense layer \n",
    "        #with the dot product of the weights and the inputs\n",
    "        return np.dot(self.weights, self.input) + self.bias\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        #Calculate weights gradient by dot product of output gradient and transpose of input\n",
    "        weights_gradient = np.dot(output_gradient, self.input.T)\n",
    "        #Calculating the input gradient by performing dot product of weights transpose and output gradient\n",
    "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
    "        #Updating the weights of the layer with weights gradient w.r.t. the rate = learning rate\n",
    "        self.weights -= learning_rate * weights_gradient\n",
    "        #Updating the bias of the layer with output gradient w.r.t. rate = learning rate\n",
    "        self.bias -= learning_rate * output_gradient\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19gBMiqX6e8q"
   },
   "outputs": [],
   "source": [
    "def log_loss(y_true, y_pred):\n",
    "    return np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def log_loss_prime(y_true, y_pred):\n",
    "    #log_loss_prime works the same way as log loss \n",
    "    #but is the derivative of the above function which will be used to \n",
    "    # compute layers gradients while backpropagation\n",
    "    return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yj5egUrZmu8I"
   },
   "outputs": [],
   "source": [
    "# load MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v07g2GZzhnyx"
   },
   "outputs": [],
   "source": [
    "#Function to preprocess our MNIST data\n",
    "def preprocess_data(x, y, limit): \n",
    "    #For simplicity we select only 10k images from class 0 and 1 from the dataset\n",
    "    zero_index = np.where(y == 0)[0][:limit]\n",
    "    one_index = np.where(y == 1)[0][:limit]\n",
    "    all_indices = np.hstack((zero_index, one_index))\n",
    "    all_indices = np.random.permutation(all_indices)\n",
    "    x, y = x[all_indices], y[all_indices]\n",
    "    #Reshape as to keep first dimension, the selected images only(i.e. 10k in our case)\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    #Normalize all pixel values[0-1], \n",
    "    #dividing by 255 because maxiumum possible pixel RGB value can be 255            \n",
    "    x = x.astype(\"float32\") / 255\n",
    "    #One hot encode all the labels \n",
    "    y = np_utils.to_categorical(y)\n",
    "    # print(y.shape)\n",
    "    y = y.reshape(len(y), 2, 1)\n",
    "    # print(y.shape)\n",
    "    return x, y\n",
    "    \n",
    "\n",
    "\n",
    "x_train, y_train = preprocess_data(x_train, y_train, 1000)\n",
    "x_test, y_test = preprocess_data(x_test, y_test, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-Ph1C-dlili"
   },
   "outputs": [],
   "source": [
    "#Define network architecture\n",
    "network = [\n",
    "    #input_shape, kernel_size, depth\n",
    "    Convolutional((1, 28, 28), 3, 5),\n",
    "    ReLU(),\n",
    "    #input_shape, kernel_size, depth, stride\n",
    "    MaxPool((5,26,26), 2, 5, 1),\n",
    "    #input_shape, output_shape\n",
    "    Reshape((5, 25, 25), (5 * 25 * 25, 1)),\n",
    "    #input_size, output_size\n",
    "    Dense(5 * 25 * 25, 100),\n",
    "    TanH(),\n",
    "    #input_size, output_size\n",
    "    Dense(100, 2),\n",
    "\n",
    "\n",
    "    Sigmoid()\n",
    "\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4IFHadTo7Edf"
   },
   "outputs": [],
   "source": [
    "#Function to make a prediction using our neural network on a given input\n",
    "def predict(network, input):\n",
    "    output = input\n",
    "    #Performing forward pass to our network through every consecutive layer\n",
    "    for layer in network:\n",
    "        output = layer.forward(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lrbbyQE5-4W"
   },
   "outputs": [],
   "source": [
    "def train(network, loss, loss_prime, x_train, y_train, epochs = 10, learning_rate = 0.01):\n",
    "    for e in range(epochs):\n",
    "        error = 0\n",
    "        for x, y in zip(x_train, y_train):\n",
    "            #Forward pass to predict on the training data, to improve the network\n",
    "            output = predict(network, x)\n",
    "\n",
    "            #Summing the losses to optimize the network's weights and biases\n",
    "            error += loss(y, output)\n",
    "\n",
    "            #Perform backward pass through every layer by computing the gradients \n",
    "            #by reversing the network to perform backpropagation\n",
    "            grad = loss_prime(y, output)\n",
    "            for layer in reversed(network):\n",
    "                grad = layer.backward(grad, learning_rate)\n",
    "\n",
    "        error /= len(x_train)\n",
    "        print(f\"Epoch : {e + 1}/{epochs}, loss = {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2501399,
     "status": "ok",
     "timestamp": 1663579653787,
     "user": {
      "displayName": "Sshubam Verma",
      "userId": "11144779609945021915"
     },
     "user_tz": -330
    },
    "id": "rTukRJtnh8He",
    "outputId": "ba0a11e2-6fea-4214-bf6d-7305c31217e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/10, loss = 0.26764046627726223\n",
      "Epoch : 2/10, loss = 0.05975942732242665\n",
      "Epoch : 3/10, loss = 0.04066104801360387\n",
      "Epoch : 4/10, loss = 0.03460079648514884\n",
      "Epoch : 5/10, loss = 0.022773413163749447\n",
      "Epoch : 6/10, loss = 0.01432637247143842\n",
      "Epoch : 7/10, loss = 0.011648843151015846\n",
      "Epoch : 8/10, loss = 0.011095301471711921\n",
      "Epoch : 9/10, loss = 0.008878966244980503\n",
      "Epoch : 10/10, loss = 0.008815326479021739\n"
     ]
    }
   ],
   "source": [
    "#Fitting our model to the data by calling the train function\n",
    "\n",
    "train(\n",
    "    network,\n",
    "    log_loss,\n",
    "    log_loss_prime,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs = 10,\n",
    "    learning_rate = 0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMG8kQpKn-Je"
   },
   "outputs": [],
   "source": [
    "#Function to make a prediction using our neural network on a given input\n",
    "def predict(network, input):\n",
    "    output = input\n",
    "    #Performing forward pass to our network through every consecutive layer\n",
    "    for layer in network:\n",
    "        output = layer.forward(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1663579653827,
     "user": {
      "displayName": "Sshubam Verma",
      "userId": "11144779609945021915"
     },
     "user_tz": -330
    },
    "id": "KG7UDjNm20ex",
    "outputId": "25441dc1-96e5-4e8e-96ef-b67e2a4244f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1980"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77688,
     "status": "ok",
     "timestamp": 1663580700411,
     "user": {
      "displayName": "Sshubam Verma",
      "userId": "11144779609945021915"
     },
     "user_tz": -330
    },
    "id": "XrTHT7hMIARa",
    "outputId": "d694088c-a7fe-471f-a3d1-2e09837de2b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Network on Test data is 99.69696969696969 %\n"
     ]
    }
   ],
   "source": [
    "#Function to calculate Accuracy of our network on testing data\n",
    "correct = 0\n",
    "for x, y in zip(x_test, y_test):\n",
    "    output = predict(network, x)\n",
    "    #Checking if the predicted label is equal to the true label\n",
    "    #Since our output is an array of class probabilities,\n",
    "    #we select the maximum value index by using the function 'numpy.argmax()'\n",
    "    #which gives us the class label\n",
    "\n",
    "    if np.equal(np.argmax(output), np.argmax(y)):\n",
    "       correct += 1\n",
    "\n",
    "print(f\"Accuracy of the Network on Test data is {(correct/len(x_test)) * 100} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-w_nu0ilcEgo"
   },
   "outputs": [],
   "source": [
    "#Test the network on a random sample from the mnist test data\n",
    "def test_random_sample():    \n",
    "    sample_index = random.randint(0, 100)\n",
    "    x_sample, y_sample = x_test[sample_index], y_test[sample_index]\n",
    "    output = predict(network, x_sample)\n",
    "    x_sample = np.squeeze(x_sample, axis = 0)\n",
    "    plt.rcParams[\"figure.figsize\"] = [2, 2]\n",
    "    plt.imshow(x_sample)\n",
    "    plt.axis('off')\n",
    "    print(f\"Predicting on image sample {sample_index} from test data\")\n",
    "    print(f\"Prediction : {np.argmax(output)}\")\n",
    "    print(f\"Actual : {np.argmax(y_sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1663579720065,
     "user": {
      "displayName": "Sshubam Verma",
      "userId": "11144779609945021915"
     },
     "user_tz": -330
    },
    "id": "XgiXJJ9uTwDs",
    "outputId": "503c50e5-4535-4269-9748-eef598ebbd64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on image sample 87 from test data\n",
      "Prediction : 0\n",
      "Actual : 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAE30lEQVR4nO2db2hVdRzGz7l3zan3mnNuqOxW5lyNWckyMdkLoS4oDQLplkiroD/qNBJf9CKCXlm02VCMMlYRRvhioFIiiUwqaCFuUtAfbTZXKzeHijbRMe89p3cXn7POuXe7/87d83xenYfzO/f+2Od+9909+51zTNu2DcFBoNATEPlDsomQbCIkmwjJJkKyiSjx2hkNxPS9rMg4bnWabvtU2URINhGSTYRkEyHZREg2EZJNhGQTIdlESDYRkk2E57lxFup68cewe2EP5P3/zof8xf3VOZ9TLlBlEyHZREg2ETQ9Ozj3zuS2Oa8c9u1eeBhywrYgl5oJyIFwGLI1OpqNKeYcVTYRkk0Eza/xs2/VJbd/f/oDx17vz3wsdBnyrg3LIFd0/JDR3PKFKpsIySZCsomYtj17pGU15JNPtd2WZmb02qHYEOTgoQrIiUvY4/2CKpsIySZCsomYNj27JIL/dnxnxyeQywOZ9enb6ao/CLkx2gJ5zgH1bFFgJJsIySZi2vTsvq0RyNGZN/P23pWbByAnvlkAOT40nLe5eKHKJkKyiZBsIoq2Z481rYT87cY2x4hZab9W+5V7IXd0roW8eM0A5K9qj0A+VHMU8vLmbZAXtapnizwj2URINhFF27OHGoOQq4Lp92gnXY8vhXzXcDdk68t6PABb9gQi6wYgJ1qnOrPsosomQrKJkGwiiqZnD76Ja8pOPfueY8SMtF+r5tgrkGuHe1xGTo11Vb9A/jryAOT44N9Zfb90UWUTIdlESDYRRdOza6L9kENm+j3aMAzjjZGG5Hbdjj7Yl3AOdmD/dAbyYy9vgtzV8RHklrnnIe9rfgJy9dvq2SLHSDYRkk2Eb3u2WYJTKwveghw0vT+nN6xxyCf2Pprcnnd1ktdTW9jVZ/82AvnzUVxz9sIc3B9uxFwoVNlESDYRvv01Hm98EPKBxR9DTqR4LlFD94uQ7/40e7fCiJ//E/LBiw2Qm8O4TMmyXR/Ik1dU2URINhGSTYRve/ZAU2lGx4eOhbI0k4kEZuESqMoZ1z3HX7k2G3K5y7hco8omQrKJkGwifNuzw/2ZfQ4re69BtlzGTYWrT+I5gCMRvD3mH3G8XLimPQ65UI8uVmUTIdlESDYRvu3ZFb+OZXT82e14K6z7Npclt62xyb32+NpHIL+7c5/n+O39Mch2z8+Ter9cocomQrKJkGwifNuzM6Uv2gH59e9XJLevJ7Cfn/5wOeSGLT9Cfm4+LhVelWIV85m/cJnSUuOC9wF5QpVNhGQTIdlE+LZnl54+B/nVC3jJ7t5FeCuMVLQucL8sN7gT16c5n9iXit5xXGpc+z4uey7UuXAnqmwiJJsIySbCtz3brMCVWg+HThZoJqnZ1PYa5KpTk/t7Il+osomQbCIkmwjf9mzn9VTt+9dDvuclPF+9pgy/2+aSZd3P41w+w3Pp2Vzvlk1U2URINhGSTYRp2+5nbqOBmF9O607AXv0Q5HMbyyAfbtoDuf4O92vHnLfsaPlnFeTvBpdArn4Gb61l38JbehSS41an68XgqmwiJJsIySaiaHu2+H/Us4VhGJJNhWQTIdlESDYRkk2EZBMh2URINhGSTYRkEyHZREg2EZJNhGQTIdlESDYRkk2E57IkMb1QZRMh2URINhGSTYRkEyHZRPwHhH3qwzbktuUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_random_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1663580700411,
     "user": {
      "displayName": "Sshubam Verma",
      "userId": "11144779609945021915"
     },
     "user_tz": -330
    },
    "id": "Nbum13oKTwDs",
    "outputId": "6442435e-2cd0-49f7-e467-6bb4b2b8089b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on image sample 56 from test data\n",
      "Prediction : 1\n",
      "Actual : 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAACsElEQVR4nO3doU8bURzA8VcyhcCS1DQEDCjcIEtwBD2x4NBYDH8EBtWEWRxBoMlcFzLmpjYzQjAk2IlJDkd6r7RN+wrce9/vR3EQmku++fXlrvegVVVVEMPce5+A3o6xQYwNYmwQY4MYG+TDqB9uz33xuiwz3x7PW8N+5mSDGBvE2CDGBjE2iLFBjA1ibBBjgxgbxNggxgYxNoixQYwNMvLzbKq/xxu145vdk9rxTnv9LU9nZpxsEGOD+Db+gvhtO7b4Y6F2/LD57zVPZ2acbBBjgxgbxDU7DK7B45x2erXj5eP92vHKwXXyOb0GJxvE2CDGBnHNDoNrcKmcbBBjgxgbBLlmxx9hhvAr6fXavTx2NjvZIMYGMTYIcs3+tPE76feXz6J74RfNvBcec7JBjA1ibBDMmv3/88fnr087X5Neq6mfV4/jZIMYG8TYIJg1+35r6F95HGvvbiv6Th7PicecbBBjgxgbpNg1u/+6OoTx+7dGubpeqx2vBK+z1XDGBjE2SLFr9vfu9Pe/4+vqXO+Fx5xsEGODFPM2Hl9qpTweXMqlVszJBjE2iLFBilmzlw7/zOy1ctnOMyknG8TYIMYGyXbNjrfdXnam/wgzvj06f/Fz6tdqMicbxNggxgbJds1O3XbbL5c/IZ3KyQYxNoixQbJZs+PPq1O23ZaynWdSTjaIsUGMDZLNmp3yaHAI9XWacl0dc7JBjA1ibJBs1uxU/c+Cl/Ic+KScbBBjgxgbpLFrdurerVK33aZwskGMDWJskMau2aluj1Zrx/OhzGfBJ+FkgxgbxNggjV2zB/ZbdRN/X042ibFBGvs2Httpr7/3KWTPyQYxNoixQYwNYmwQY4MYG8TYIMYGMTaIsUGMDWJsEGODGBukVVVl/osEDXKyQYwNYmwQY4MYG8TYIE/KM2AD0dLjaAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_random_sample()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1LUGoRCcbqWsydpBm7Dvgs7U16UPjlAt5",
     "timestamp": 1663576808311
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c410295bfbef04d36b350637c05ef8df3a08239fed6bc74d3119fe46a0288bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
